# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
## cont_TDM <- removeSparseTerms(cont_TDM,0.7)
# remove the sparse terms in the TDM (higher number means lower threshold)
cleanCharacter <- function(char){
char.cl <- tolower(char) # Transform the capital letters to lower forms
char.cl <- union(char.cl,char.cl) # Take the same words as one
result <- char.cl
}
# Create the dictionary for stemmed words completion
# Words are from http://www.talkenglish.com/vocabulary
stemdic <- read.csv('eng_dictionary.csv')
stemdic_word <- stemdic[,1]
stemdic_word <- as.character(stemdic_word)
stemdic_word.cl <- cleanCharacter(stemdic_word)
# Complete the stemmed words
thirdname <- stemCompletion(midname,dictionary = cont.cl,type = 'prevalent')
thirdname[thirdname==''] <- midname[thirdname=='']
fourthname <- stemDocument(thirdname)
compname <- stemCompletion(fourthname,dictionary = stemdic_word.cl,type = 'prevalent')
rownames(cont_TDM)[compname==''] <- thirdname[compname=='']
rownames(cont_TDM)[!compname=='']<- compname[!compname=='']
finalname <- rownames(cont_TDM)
contmtx <- as.matrix(cont_TDM)
# Choose the most frequent words in each document and write in a file
sink('Contract frequent word_stem_and_completed.txt')
for (i in 1:ncol(contmtx)){
temp <- contmtx[,i]
freqtemp <- names(which(temp>=3))
cat(i,freqtemp,'\n')
}
sink()
# Draw the wordcloud
rsum <- rowSums(contmtx)
wordcloud(names(rsum[rsum>=10]),rsum[rsum>=10],colors = c('green','blue','yellow','red'),random.order = TRUE)
# Draw the Association graph between frequent words
freq.terms <- findFreqTerms(cont_TDM,lowfreq = 15)
plot(cont_TDM,term = freq.terms,corThreshold = 0.5,weighting = T)
# The freq.terms here should not be too much, or the plotting will fail
# Write the matrix into a csv file
## write.csv(t(contmtx),'Contract frequent matrix_stemmed_and_completed.csv')
# Find the words that only appear in one document
sink('Words in only one document.txt')
for (i in 1:nrow(contmtx)){
tpvec <- contmtx[i,]
if (length(tpvec[tpvec==0]) >= 10){
cat(i,rownames(contmtx)[i],'\n')
}
}
sink()
# Find the words that appear few in all documents
sparseWords <- finalname[rsum <= 2]
#Init
rm(list=ls())
libs <-c('tm','plyr','class')
lapply(libs,require,character.only = TRUE) # If write 'library',only the first character
# 'tm' will be inputted
#Set options
options(stringsAsFactors = FALSE)
#Set path
ctrtype <-c('House Rental','Car') # Two types to be classified in
pathname <-'D:/Desktop/R work/Test Document' # Name of the path containing two type folders
#Corpus cleaning
cleanCorpus <- function(corpus){
corpus <- tm_map(corpus,removePunctuation)
corpus <- tm_map(corpus,stripWhitespace)
corpus <- tm_map(corpus,content_transformer(tolower)) # If we only write 'tolower' here, the format will
# be changed to characters rather than corpus
corpus <- tm_map(corpus,removeWords,stopwords('english'))
corpus <- tm_map(corpus,removeNumbers)
}
#Generate TDM of the speeches
generate_TDM <- function(type,path){
s.dir <- sprintf('%s/%s',path,type)
# sprintf is a function that outputs two strings of paths of each folder
s.cor <- Corpus(DirSource(s.dir),readerControl = list(language = 'en'))
# Read in all documents in both folders to the Corpus
s.cor <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor)
s.tdm <- removeSparseTerms(s.tdm,0.7) # Remove the words with low frequency (but not know how to measure it)
result <- list(tdm = s.tdm,name = type) # Output a list of tdm and name
}
tdm <- lapply(ctrtype,generate_TDM,pathname) # lapply is a function that outputs a vector
# with the size of the first variable (ctrtype)
#Attach name to each row of TDM
bindContract_TDM <- function(tdm){
s.mat <- t(data.matrix(tdm[['tdm']]))
s.df <- as.data.frame(s.mat, stringsAsFactors = FALSE)
s.df <- cbind(s.df,rep(tdm[['name']],nrow(s.df)))
colnames(s.df)[ncol(s.df)]<-'targetcontract'
# Let the final column be the type name of each document
result <- s.df
}
ctrtTDM <- lapply(tdm,bindContract_TDM)
#Stack
tdm.stack <- do.call(rbind.fill,ctrtTDM) # do.call is a function that runs a function on
# multiple variables (but only one here)
tdm.stack[is.na(tdm.stack)] <- 0 # Let all the NA in tdm.stack be 0
#Setting train and test samples
train.idx <- sample(nrow(tdm.stack),ceiling(nrow(tdm.stack)*0.5)) # Take 50% of all documents to be trainers
test.idx <- (1:nrow(tdm.stack))[-train.idx] # All the others are testers
#K-nearest-neighbour (knn) Model
tdm.ctrt <- tdm.stack[,'targetcontract']
tdm.stack.nl <- tdm.stack[,!colnames(tdm.stack) %in% 'targetcontract'] # Leave out the 'targetcontract' column
knn.pred <- knn(tdm.stack.nl[train.idx,],tdm.stack.nl[test.idx,],tdm.ctrt[train.idx],k=ceiling(sqrt(nrow(tdm.stack))))
# Run the Knn Algorithm with trainers and testers, with the known types of the trainers
#Accuracy measurement
conf.mat <- table('prediction'= knn.pred, Actual = tdm.ctrt[test.idx])
# Create a matrix of comparison between prediction and actual results
scsrate <- sum(diag(conf.mat))/length(test.idx)
# Measuring the success rate of the test
conf.mat;scsrate
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
## cont_TDM <- removeSparseTerms(cont_TDM,0.7)
# remove the sparse terms in the TDM (higher number means lower threshold)
cleanCharacter <- function(char){
char.cl <- tolower(char) # Transform the capital letters to lower forms
char.cl <- union(char.cl,char.cl) # Take the same words as one
result <- char.cl
}
# Create the dictionary for stemmed words completion
# Words are from http://www.talkenglish.com/vocabulary
stemdic <- read.csv('eng_dictionary.csv')
stemdic_word <- stemdic[,1]
stemdic_word <- as.character(stemdic_word)
stemdic_word.cl <- cleanCharacter(stemdic_word)
# Complete the stemmed words
thirdname <- stemCompletion(midname,dictionary = cont.cl,type = 'prevalent')
thirdname[thirdname==''] <- midname[thirdname=='']
fourthname <- stemDocument(thirdname)
compname <- stemCompletion(fourthname,dictionary = stemdic_word.cl,type = 'prevalent')
rownames(cont_TDM)[compname==''] <- thirdname[compname=='']
rownames(cont_TDM)[!compname=='']<- compname[!compname=='']
finalname <- rownames(cont_TDM)
contmtx <- as.matrix(cont_TDM)
# Choose the most frequent words in each document and write in a file
sink('Contract frequent word_stem_and_completed.txt')
for (i in 1:ncol(contmtx)){
temp <- contmtx[,i]
freqtemp <- names(which(temp>=3))
cat(i,freqtemp,'\n')
}
sink()
# Draw the wordcloud
rsum <- rowSums(contmtx)
wordcloud(names(rsum[rsum>=10]),rsum[rsum>=10],colors = c('green','blue','yellow','red'),random.order = TRUE)
# Draw the Association graph between frequent words
freq.terms <- findFreqTerms(cont_TDM,lowfreq = 15)
plot(cont_TDM,term = freq.terms,corThreshold = 0.5,weighting = T)
# The freq.terms here should not be too much, or the plotting will fail
# Write the matrix into a csv file
## write.csv(t(contmtx),'Contract frequent matrix_stemmed_and_completed.csv')
# Find the words that only appear in one document
sink('Words in only one document.txt')
for (i in 1:nrow(contmtx)){
tpvec <- contmtx[i,]
if (length(tpvec[tpvec==0]) >= 10){
cat(i,rownames(contmtx)[i],'\n')
}
}
sink()
# Find the words that appear few in all documents
sparseWords <- finalname[rsum <= 2]
rm(list=ls())
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
## cont_TDM <- removeSparseTerms(cont_TDM,0.7)
# remove the sparse terms in the TDM (higher number means lower threshold)
# cleanCharacter <- function(char){
#   char.cl <- tolower(char) # Transform the capital letters to lower forms
#   char.cl <- union(char.cl,char.cl) # Take the same words as one
#
#   result <- char.cl
# }
#
# # Create the dictionary for stemmed words completion
# # Words are from http://www.talkenglish.com/vocabulary
# stemdic <- read.csv('eng_dictionary.csv')
# stemdic_word <- stemdic[,1]
# stemdic_word <- as.character(stemdic_word)
# stemdic_word.cl <- cleanCharacter(stemdic_word)
#
# # Complete the stemmed words
# thirdname <- stemCompletion(midname,dictionary = cont.cl,type = 'prevalent')
# thirdname[thirdname==''] <- midname[thirdname=='']
# fourthname <- stemDocument(thirdname)
# compname <- stemCompletion(fourthname,dictionary = stemdic_word.cl,type = 'prevalent')
# rownames(cont_TDM)[compname==''] <- thirdname[compname=='']
# rownames(cont_TDM)[!compname=='']<- compname[!compname=='']
# finalname <- rownames(cont_TDM)
#
# contmtx <- as.matrix(cont_TDM)
#
# # Choose the most frequent words in each document and write in a file
# sink('Contract frequent word_stem_and_completed.txt')
# for (i in 1:ncol(contmtx)){
#   temp <- contmtx[,i]
#   freqtemp <- names(which(temp>=3))
#   cat(i,freqtemp,'\n')
# }
# sink()
#
# # Draw the wordcloud
# rsum <- rowSums(contmtx)
# wordcloud(names(rsum[rsum>=10]),rsum[rsum>=10],colors = c('green','blue','yellow','red'),random.order = TRUE)
#
# # Draw the Association graph between frequent words
# freq.terms <- findFreqTerms(cont_TDM,lowfreq = 15)
# plot(cont_TDM,term = freq.terms,corThreshold = 0.5,weighting = T)
# # The freq.terms here should not be too much, or the plotting will fail
#
# # Write the matrix into a csv file
# ## write.csv(t(contmtx),'Contract frequent matrix_stemmed_and_completed.csv')
#
# # Find the words that only appear in one document
# sink('Words in only one document.txt')
# for (i in 1:nrow(contmtx)){
#     tpvec <- contmtx[i,]
#     if (length(tpvec[tpvec==0]) >= 10){
#     cat(i,rownames(contmtx)[i],'\n')
#    }
#   }
# sink()
#
# # Find the words that appear few in all documents
# sparseWords <- finalname[rsum <= 2]
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
## cont_TDM <- removeSparseTerms(cont_TDM,0.7)
# remove the sparse terms in the TDM (higher number means lower threshold)
# cleanCharacter <- function(char){
#   char.cl <- tolower(char) # Transform the capital letters to lower forms
#   char.cl <- union(char.cl,char.cl) # Take the same words as one
#
#   result <- char.cl
# }
#
# # Create the dictionary for stemmed words completion
# # Words are from http://www.talkenglish.com/vocabulary
# stemdic <- read.csv('eng_dictionary.csv')
# stemdic_word <- stemdic[,1]
# stemdic_word <- as.character(stemdic_word)
# stemdic_word.cl <- cleanCharacter(stemdic_word)
#
# # Complete the stemmed words
# thirdname <- stemCompletion(midname,dictionary = cont.cl,type = 'prevalent')
# thirdname[thirdname==''] <- midname[thirdname=='']
# fourthname <- stemDocument(thirdname)
# compname <- stemCompletion(fourthname,dictionary = stemdic_word.cl,type = 'prevalent')
# rownames(cont_TDM)[compname==''] <- thirdname[compname=='']
# rownames(cont_TDM)[!compname=='']<- compname[!compname=='']
# finalname <- rownames(cont_TDM)
#
# contmtx <- as.matrix(cont_TDM)
#
# # Choose the most frequent words in each document and write in a file
# sink('Contract frequent word_stem_and_completed.txt')
# for (i in 1:ncol(contmtx)){
#   temp <- contmtx[,i]
#   freqtemp <- names(which(temp>=3))
#   cat(i,freqtemp,'\n')
# }
# sink()
#
# # Draw the wordcloud
# rsum <- rowSums(contmtx)
# wordcloud(names(rsum[rsum>=10]),rsum[rsum>=10],colors = c('green','blue','yellow','red'),random.order = TRUE)
#
# # Draw the Association graph between frequent words
# freq.terms <- findFreqTerms(cont_TDM,lowfreq = 15)
# plot(cont_TDM,term = freq.terms,corThreshold = 0.5,weighting = T)
# # The freq.terms here should not be too much, or the plotting will fail
#
# # Write the matrix into a csv file
# ## write.csv(t(contmtx),'Contract frequent matrix_stemmed_and_completed.csv')
#
# # Find the words that only appear in one document
# sink('Words in only one document.txt')
# for (i in 1:nrow(contmtx)){
#     tpvec <- contmtx[i,]
#     if (length(tpvec[tpvec==0]) >= 10){
#     cat(i,rownames(contmtx)[i],'\n')
#    }
#   }
# sink()
#
# # Find the words that appear few in all documents
# sparseWords <- finalname[rsum <= 2]
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "D:/Desktop/R work/Personal Contract"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "G:/Codes/Text_Mining/Additional test documents all"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
write.csv(cont_TDM,'TermDocumentMatrix.csv')
cont_TDM
rm(list=ls())
libs <- c('tm','wordcloud','Rgraphviz')
lapply(libs,require, character.only = TRUE)
options(stringsAsFactors = FALSE)
# Define the path and Term Document Matrix
path <- "G:/Codes/Text_Mining/Additional test documents all"
cont <- Corpus(DirSource(path),readerControl = list(language='en'))
myStpwdlist <- c('shall','will','due','without','john','jane','doe')
#clean the corpus
cleanCorpus <- function(corpus){
corpus_cl <- tm_map(corpus,content_transformer(gsub),pattern = ',',replacement = ' ')
corpus_cl <- tm_map(corpus_cl,removePunctuation)
corpus_cl <- tm_map(corpus_cl,stripWhitespace)
corpus_cl <- tm_map(corpus_cl,content_transformer(tolower))
corpus_cl <- tm_map(corpus_cl,removeWords,c(myStpwdlist,stopwords('english')))
corpus_cl <- tm_map(corpus_cl,removeNumbers)
result <- corpus_cl
}
# Stem the words in the corpus
cont.cl <- cleanCorpus(cont)
cont.cl.st <- tm_map(cont.cl,stemDocument)
cont_TDM <- TermDocumentMatrix(cont.cl.st)
midname <- rownames(cont_TDM)
write.csv(cont_TDM,'TermDocumentMatrix.csv')
## cont_TDM <- removeSparseTerms(cont_TDM,0.7)
# remove the sparse terms in the TDM (higher number means lower threshold)
# cleanCharacter <- function(char){
#   char.cl <- tolower(char) # Transform the capital letters to lower forms
#   char.cl <- union(char.cl,char.cl) # Take the same words as one
#
#   result <- char.cl
# }
#
# # Create the dictionary for stemmed words completion
# # Words are from http://www.talkenglish.com/vocabulary
# stemdic <- read.csv('eng_dictionary.csv')
# stemdic_word <- stemdic[,1]
# stemdic_word <- as.character(stemdic_word)
# stemdic_word.cl <- cleanCharacter(stemdic_word)
#
# # Complete the stemmed words
# thirdname <- stemCompletion(midname,dictionary = cont.cl,type = 'prevalent')
# thirdname[thirdname==''] <- midname[thirdname=='']
# fourthname <- stemDocument(thirdname)
# compname <- stemCompletion(fourthname,dictionary = stemdic_word.cl,type = 'prevalent')
# rownames(cont_TDM)[compname==''] <- thirdname[compname=='']
# rownames(cont_TDM)[!compname=='']<- compname[!compname=='']
# finalname <- rownames(cont_TDM)
#
# contmtx <- as.matrix(cont_TDM)
#
# # Choose the most frequent words in each document and write in a file
# sink('Contract frequent word_stem_and_completed.txt')
# for (i in 1:ncol(contmtx)){
#   temp <- contmtx[,i]
#   freqtemp <- names(which(temp>=3))
#   cat(i,freqtemp,'\n')
# }
# sink()
#
# # Draw the wordcloud
# rsum <- rowSums(contmtx)
# wordcloud(names(rsum[rsum>=10]),rsum[rsum>=10],colors = c('green','blue','yellow','red'),random.order = TRUE)
#
# # Draw the Association graph between frequent words
# freq.terms <- findFreqTerms(cont_TDM,lowfreq = 15)
# plot(cont_TDM,term = freq.terms,corThreshold = 0.5,weighting = T)
# # The freq.terms here should not be too much, or the plotting will fail
#
# # Write the matrix into a csv file
# ## write.csv(t(contmtx),'Contract frequent matrix_stemmed_and_completed.csv')
#
# # Find the words that only appear in one document
# sink('Words in only one document.txt')
# for (i in 1:nrow(contmtx)){
#     tpvec <- contmtx[i,]
#     if (length(tpvec[tpvec==0]) >= 10){
#     cat(i,rownames(contmtx)[i],'\n')
#    }
#   }
# sink()
#
# # Find the words that appear few in all documents
# sparseWords <- finalname[rsum <= 2]
write.csv(cont_TDM, file = "TermDocumentMatrix.csv")
cont_TDM[1]
cont_TDM[[1]]
